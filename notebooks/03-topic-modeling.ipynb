{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf5f7cd9",
   "metadata": {},
   "source": [
    "# Topic modeling with gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "072135ac",
   "metadata": {},
   "source": [
    "::::{important}\n",
    "This notebooks is based on the blog post [Topic Modeling with Gensim (Python)] and [Topic modeling visualization – How to present the results of LDA models?](https://www.machinelearningplus.com/nlp/topic-modeling-visualization-how-to-present-results-lda-models)(https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/) by **Selva Prabhakaran** published in www.machinelearningplus.com\n",
    "::::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5542e086",
   "metadata": {},
   "source": [
    "*Topic Modeling is a technique to extract the hidden topics from large volumes of text. Latent Dirichlet Allocation(LDA) is a popular algorithm for topic modeling with excellent implementations in the Python’s Gensim package. The challenge, however, is how to **extract good quality of topics that are clear, segregated and meaningful**. This depends heavily on the quality of text preprocessing and the strategy of finding the optimal number of topics. This tutorial attempts to tackle both of these problems.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "642acab9",
   "metadata": {},
   "source": [
    "## Table of contents\n",
    "\n",
    "1. Introduction\n",
    "2. Prerequisites – Download nltk stopwords and spacy model\n",
    "3. Import Packages\n",
    "4. What does LDA do?\n",
    "5. Prepare Stopwords\n",
    "6. Import Newsgroups Data\n",
    "7. Remove emails and newline characters\n",
    "8. Tokenize words and Clean-up text\n",
    "9. Creating Bigram and Trigram Models\n",
    "10. Remove Stopwords, Make Bigrams and Lemmatize\n",
    "11. Create the Dictionary and Corpus needed for Topic Modeling\n",
    "12. Building the Topic Model\n",
    "13. View the topics in LDA model\n",
    "14. Compute Model Perplexity and Coherence Score\n",
    "15. Visualize the topics-keywords\n",
    "16. Building LDA Mallet Model\n",
    "17. How to find the optimal number of topics for LDA?\n",
    "18. Finding the dominant topic in each sentence\n",
    "19. Find the most representative document for each topic\n",
    "20. Topic distribution across documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d13a1d1",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "One of the primary applications of natural language processing is to automatically extract what topics people are discussing from large volumes of text. Some examples of large text could be feeds from social media, customer reviews of hotels, movies, etc, user feedbacks, news stories, e-mails of customer complaints etc.\n",
    "\n",
    "Knowing what people are talking about and understanding their problems and opinions is highly valuable to businesses, administrators, political campaigns. And it’s really hard to manually read through such large volumes and compile the topics.\n",
    "\n",
    "\n",
    "Thus is required an automated algorithm that can read through the text documents and automatically output the topics discussed.\n",
    "\n",
    "In this tutorial, we will take a real example of the ’20 Newsgroups’ dataset and use LDA to extract the naturally discussed topics.\n",
    "\n",
    "I will be using the Latent Dirichlet Allocation (LDA) from Gensim package along with the Mallet’s implementation (via Gensim). Mallet has an efficient implementation of the LDA. It is known to run faster and gives better topics segregation.\n",
    "\n",
    "We will also extract the volume and percentage contribution of each topic to get an idea of how important a topic is.\n",
    "\n",
    "Let’s begin!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f40a82f5",
   "metadata": {},
   "source": [
    "## 2. Prerequisites – Download nltk stopwords and spacy model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ce08b0",
   "metadata": {},
   "source": [
    "We will need the `stopwords` from NLTK and spacy’s en model for text pre-processing. Later, we will be using the spacy model for lemmatization.\n",
    "\n",
    "Lemmatization is nothing but converting a word to its root word. For example: the lemma of the word ‘machines’ is ‘machine’. Likewise, ‘walking’ –> ‘walk’, ‘mice’ –> ‘mouse’ and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "021532f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c55e6d9c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "475fb4a8",
   "metadata": {},
   "source": [
    "### 3. Import Packages\n",
    "\n",
    "The core packages used in this tutorial are `re`, `gensim`, `spacy` and `pyLDAvis`. \n",
    "Besides this we will also using `hvplot`, `numpy` and `pandas` for data handling and visualization. Let’s import them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5395a8d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "\n",
    "# Gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "# spacy for lemmatization\n",
    "import spacy\n",
    "\n",
    "# Plotting tools\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models as gensimvis  # don't skip this\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Enable logging for gensim - optional\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\",category=FutureWarning)\n",
    "pyLDAvis.enable_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a4df59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import holoviews as hv\n",
    "import pandas as pd\n",
    "import hvplot.pandas\n",
    "hv.extension(\"bokeh\")\n",
    "pd.options.plotting.backend = 'holoviews'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c77fcd6c",
   "metadata": {},
   "source": [
    "## 4. What does LDA do?\n",
    "\n",
    "LDA’s approach to topic modeling is it considers each document as a collection of topics in a certain proportion. And each topic as a collection of keywords, again, in a certain proportion.\n",
    "\n",
    "Once you provide the algorithm with the number of topics, all it does it to rearrange the topics distribution within the documents and keywords distribution within the topics to obtain a good composition of topic-keywords distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb1932fd",
   "metadata": {},
   "source": [
    "When I say topic, what is it actually and how it is represented?\n",
    "\n",
    "A topic is nothing but a collection of dominant keywords that are typical representatives. Just by looking at the keywords, you can identify what the topic is all about.\n",
    "\n",
    "The following are key factors to obtaining good segregation topics:\n",
    "\n",
    "1. The quality of text processing.\n",
    "2. The variety of topics the text talks about.\n",
    "3. The choice of topic modeling algorithm.\n",
    "4. The number of topics fed to the algorithm.\n",
    "5. The algorithms tuning parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a43fb2bb",
   "metadata": {},
   "source": [
    "## 5. Prepare Stopwords\n",
    "We have already downloaded the stopwords. Let’s import them and make it available in `stop_words`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd1f1875",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLTK Stop words\n",
    "from nltk.corpus import stopwords\n",
    "import string \n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['from', 'subject', 're', 'edu', 'use'] + list(string.ascii_lowercase))\n",
    "stop_words = list(sorted(list(set(stop_words))))\n",
    "stop_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22351701",
   "metadata": {},
   "source": [
    "## 6. Import Newsgroups Data\n",
    "\n",
    "We will be using the **20-Newsgroups** dataset for this exercise. This version of the dataset contains about 11k newsgroups posts from 20 different topics. This is available as newsgroups.json.\n",
    "\n",
    "This is imported using `pandas.read_json` and the resulting dataset has 3 columns as shown.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d2de6b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Dataset\n",
    "df = pd.read_json('https://raw.githubusercontent.com/selva86/datasets/master/newsgroups.json')\n",
    "print(df.target_names.unique())\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2826d393",
   "metadata": {},
   "source": [
    "## 7. Remove emails and newline characters\n",
    "As you can see there are many emails, newline and extra spaces that is quite distracting. Let’s get rid of them using regular expressions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf22815",
   "metadata": {},
   "source": [
    "After removing the emails and extra spaces, the text still looks messy. It is not ready for the LDA to consume. You need to break down each sentence into a list of words through tokenization, while clearing up all the messy text in the process.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd687d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to list\n",
    "data = df.content.values.tolist()\n",
    "\n",
    "def clean_sentence(sent):\n",
    "    sent = re.sub('\\S*@\\S*\\s?', '', sent) # Remove Emails\n",
    "    sent = re.sub('\\s+', ' ', sent) # Remove new line characters\n",
    "    sent = re.sub(\"\\'\", \"\", sent) # Remove distracting single quotes\n",
    "    return sent\n",
    "\n",
    "data = [clean_sentence(sent) for sent in data]\n",
    "pprint(data[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b031241a",
   "metadata": {},
   "source": [
    "## 8. Tokenize words and Clean-up text\n",
    "\n",
    "Let’s tokenize each sentence into a list of words, removing punctuations and unnecessary characters altogether.\n",
    "\n",
    "Gensim’s `simple_preprocess()` is great for this. Additionally I have set `deacc=True` to remove the punctuations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f75d6ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n",
    "\n",
    "data_words = list(sent_to_words(data))\n",
    "\n",
    "print(data_words[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03a1d169",
   "metadata": {},
   "source": [
    "## 9. Creating Bigram and Trigram Models\n",
    "\n",
    "Bigrams are two words frequently occurring together in the document. Trigrams are 3 words frequently occurring.\n",
    "\n",
    "Some examples in our example are: ‘front_bumper’, ‘oil_leak’, ‘maryland_college_park’ etc.\n",
    "\n",
    "\n",
    "Gensim’s Phrases model can build and implement the bigrams, trigrams, quadgrams and more. The two important arguments to Phrases are `min_count` and `threshold`. The higher the values of these param, the harder it is for words to be combined to bigrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "283cb0e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the bigram and trigram models\n",
    "bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
    "trigram = gensim.models.Phrases(bigram[data_words], threshold=100)  \n",
    "\n",
    "# Faster way to get a sentence clubbed as a trigram/bigram\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "\n",
    "# See trigram example\n",
    "print(trigram_mod[bigram_mod[data_words[0]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a68bd0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "trigram_mod.phrasegrams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68a569e4",
   "metadata": {},
   "source": [
    "## 10. Remove Stopwords, Make Bigrams and Lemmatize\n",
    "The bigrams model is ready. Let’s define the functions to remove the stopwords, make bigrams and lemmatization and call them sequentially."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5507864f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define functions for stopwords, bigrams, trigrams and lemmatization\n",
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "\n",
    "def make_bigrams(texts):\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "def make_trigrams(texts):\n",
    "    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "\n",
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac8d110f",
   "metadata": {},
   "source": [
    "Let’s call the functions in order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece59e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Stop Words\n",
    "data_words_nostops = remove_stopwords(data_words)\n",
    "\n",
    "# Form Bigrams\n",
    "#data_words_bigrams = make_bigrams(data_words_nostops)\n",
    "\n",
    "# Form Trigrams\n",
    "data_words_trigrams = make_trigrams(data_words_nostops)\n",
    "\n",
    "# Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n",
    "# python3 -m spacy download en\n",
    "nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
    "\n",
    "# Do lemmatization keeping only noun, adj, vb, adv\n",
    "data_lemmatized = lemmatization(data_words_trigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
    "\n",
    "print(data_lemmatized[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdd113ac",
   "metadata": {},
   "source": [
    "## 11. Create the Dictionary and Corpus needed for Topic Modeling\n",
    "The two main inputs to the LDA topic model are the dictionary(id2word) and the corpus. Let’s create them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a43e2e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dictionary\n",
    "id2word = corpora.Dictionary(data_lemmatized)\n",
    "\n",
    "# Create Corpus\n",
    "texts = data_lemmatized\n",
    "\n",
    "# Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in texts]\n",
    "\n",
    "# View\n",
    "print(corpus[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ab898d",
   "metadata": {},
   "source": [
    "Gensim creates a unique id for each word in the document. The produced corpus shown above is a mapping of (word_id, word_frequency).\n",
    "\n",
    "For example, (0, 1) above implies, word id 0 occurs once in the first document. Likewise, word id 1 occurs twice and so on.\n",
    "\n",
    "This is used as the input by the LDA model.\n",
    "\n",
    "If you want to see what word a given id corresponds to, pass the id as a key to the dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b6a35f",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(id2word.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a29a91",
   "metadata": {},
   "source": [
    "Or, you can see a human-readable form of the corpus itself.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e20c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Human readable format of corpus (term-frequency)\n",
    "[[(id2word[id], freq) for id, freq in cp] for cp in corpus[:1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b701e9",
   "metadata": {},
   "source": [
    "Alright, without digressing further let’s jump back on track with the next step: Building the topic model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e3adb93",
   "metadata": {},
   "source": [
    "## 12. Building the Topic Model\n",
    "We have everything required to train the LDA model. In addition to the corpus and dictionary, you need to provide the number of topics as well.\n",
    "\n",
    "Apart from that, alpha and eta are hyperparameters that affect sparsity of the topics. According to the Gensim docs, both defaults to 1.0/num_topics prior.\n",
    "\n",
    "chunksize is the number of documents to be used in each training chunk. update_every determines how often the model parameters should be updated and passes is the total number of training passes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "436be240",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build LDA model\n",
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=20, \n",
    "                                           random_state=100,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=100,\n",
    "                                           passes=10,\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "557d47f6",
   "metadata": {},
   "source": [
    "## 13. View the topics in LDA model\n",
    "The above LDA model is built with 20 different topics where each topic is a combination of keywords and each keyword contributes a certain weightage to the topic.\n",
    "\n",
    "You can see the keywords for each topic and the weightage(importance) of each keyword using lda_model.print_topics() as shown next.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d526992",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_matrix = lda_model.get_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "769ff623",
   "metadata": {},
   "outputs": [],
   "source": [
    "hv.Image((topic_matrix @ topic_matrix.T)).opts(cnorm='log', tools=[\"hover\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a6e5da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the Keyword in the 10 topics\n",
    "pprint(lda_model.print_topics())\n",
    "doc_lda = lda_model[corpus]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e690a97b",
   "metadata": {},
   "source": [
    "How to interpret this?\n",
    "\n",
    "Topic 0 is a represented as _0.016“car” + 0.014“power” + 0.010“light” + 0.009“drive” + 0.007“mount” + 0.007“controller” + 0.007“cool” + 0.007“engine” + 0.007“back” + ‘0.006“turn”.\n",
    "\n",
    "It means the top 10 keywords that contribute to this topic are: ‘car’, ‘power’, ‘light’.. and so on and the weight of ‘car’ on topic 0 is 0.016.\n",
    "\n",
    "The weights reflect how important a keyword is to that topic.\n",
    "\n",
    "Looking at these keywords, can you guess what this topic could be? You may summarise it either are ‘cars’ or ‘automobiles’.\n",
    "\n",
    "Likewise, can you go through the remaining topic keywords and judge what the topic is?\n",
    "\n",
    "![topics_from_keywords](images/nlp/topics_from_keywords.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "337af7a4",
   "metadata": {},
   "source": [
    "## What is the Dominant topic and its percentage contribution in each document\n",
    "In LDA models, each document is composed of multiple topics. But, typically only one of the topics is dominant. The below code extracts this dominant topic for each sentence and shows the weight of the topic and the keywords in a nicely formatted output.\n",
    "\n",
    "This way, you will know which document belongs predominantly to which topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deadd601",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_topics_sentences(ldamodel=None, corpus=corpus, texts=data):\n",
    "    # Init output\n",
    "    sent_topics_df = pd.DataFrame()\n",
    "\n",
    "    # Get main topic in each document\n",
    "    for i, row_list in enumerate(ldamodel[corpus]):\n",
    "        row = row_list[0] if ldamodel.per_word_topics else row_list            \n",
    "        # print(row)\n",
    "        row = sorted(row, key=lambda x: (x[1]), reverse=True)\n",
    "        # Get the Dominant topic, Perc Contribution and Keywords for each document\n",
    "        for j, (topic_num, prop_topic) in enumerate(row):\n",
    "            if j == 0:  # => dominant topic\n",
    "                wp = ldamodel.show_topic(topic_num)\n",
    "                topic_keywords = \", \".join([word for word, prop in wp])\n",
    "                sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
    "            else:\n",
    "                break\n",
    "    sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']\n",
    "\n",
    "    # Add original text to the end of the output\n",
    "    contents = pd.Series(texts)\n",
    "    sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)\n",
    "    return(sent_topics_df)\n",
    "\n",
    "\n",
    "df_topic_sents_keywords = format_topics_sentences(ldamodel=lda_model, corpus=corpus, texts=data_lemmatized)\n",
    "\n",
    "# Format\n",
    "df_dominant_topic = df_topic_sents_keywords.reset_index()\n",
    "df_dominant_topic.columns = ['Document_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Text']\n",
    "df_dominant_topic.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "389aca9a",
   "metadata": {},
   "source": [
    "## The most representative sentence for each topic\n",
    "Sometimes you want to get samples of sentences that most represent a given topic. This code gets the most exemplar sentence for each topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22fbe0b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display setting to show more characters in column\n",
    "pd.options.display.max_colwidth = 100\n",
    "\n",
    "sent_topics_sorteddf_mallet = pd.DataFrame()\n",
    "sent_topics_outdf_grpd = df_topic_sents_keywords.groupby('Dominant_Topic')\n",
    "\n",
    "for i, grp in sent_topics_outdf_grpd:\n",
    "    sent_topics_sorteddf_mallet = pd.concat([sent_topics_sorteddf_mallet, \n",
    "                                             grp.sort_values(['Perc_Contribution'], ascending=False).head(1)], \n",
    "                                            axis=0)\n",
    "\n",
    "# Reset Index    \n",
    "sent_topics_sorteddf_mallet.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Format\n",
    "sent_topics_sorteddf_mallet.columns = ['Topic_Num', \"Topic_Perc_Contrib\", \"Keywords\", \"Representative Text\"]\n",
    "\n",
    "# Show\n",
    "sent_topics_sorteddf_mallet.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff825d6",
   "metadata": {},
   "source": [
    "## Frequency Distribution of Word Counts in Documents\n",
    "\n",
    "When working with a large number of documents, you want to know how big the documents are as a whole and by topic. Let’s plot the document word counts distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a4e5f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_word_counts.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b802a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_word_counts = df_dominant_topic[\"Text\"].map(len).to_frame()\n",
    "df_word_counts = df_word_counts.rename(columns={\"Text\": \"Number of Words\"}, index={0:\"Text\"})\n",
    "quantile_99 = np.quantile(df_word_counts.values, q=0.99)\n",
    "histogram_bins = np.linspace(0, 600, 100)\n",
    "df_word_counts.hist(bins=histogram_bins).opts(title=\"Distribution of Document Word Counts\",\n",
    "                                              xlim=(0, 602),\n",
    "                                              tools=[\"hover\"],\n",
    "                                              ylabel=\"Number of documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a2b75c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counts_topic = df_dominant_topic.groupby(\"Dominant_Topic\")[\"Text\"].apply(lambda x: [len(d) for d in x]).to_frame()\n",
    "word_counts_topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ac0a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counts_topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bab97d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from panel.interact import interact, fixed\n",
    "import panel as pn\n",
    "def plot_word_count_topic(i, df, bins=25, density=False):\n",
    "    x = np.histogram(df.loc[i].values[0], bins=bins, density=density)\n",
    "    return hv.Histogram(x).opts(tools=[\"hover\"])\n",
    "\n",
    "interact(plot_word_count_topic, i=np.arange(20), df=fixed(word_counts_topic), bins=fixed(histogram_bins), density=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb4e1f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "w = pn.widgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a4c49c",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_selections = {f\"Topic {v}\": v for v in word_counts_topic.index.values}\n",
    "widget= pn.widgets.MultiSelect(options=topic_selections, value=list(topic_selections.values()), name=\"Select topics\")\n",
    "widget.height=275"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "487f5523",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_topic_distributions(topics, density=False):\n",
    "    plot = hv.NdOverlay({i:plot_word_count_topic(i, word_counts_topic, histogram_bins, density).opts(alpha=0.6) for i in topics})\n",
    "    return plot.opts(xlim=(0, 600), width=800, height=400, legend_position=\"top\")\n",
    "\n",
    "widgets, plot = interact(plot_topic_distributions, topics=widget)\n",
    "pn.Row(widgets, plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec6f1679",
   "metadata": {},
   "source": [
    "## Word Clouds of Top N Keywords in Each Topic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b15410",
   "metadata": {},
   "source": [
    "Though you’ve already seen what are the topic keywords in each topic, a word cloud with the size of the words proportional to the weight is a pleasant sight. The coloring of the topics I’ve taken here is followed in the subsequent plots as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce98a3f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Wordcloud of Top N words in each topic\n",
    "from matplotlib import pyplot as plt\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "cols = [color for name, color in mcolors.TABLEAU_COLORS.items()]  # more colors: 'mcolors.XKCD_COLORS'\n",
    "\n",
    "cloud = WordCloud(stopwords=stop_words,\n",
    "                  background_color='white',\n",
    "                  width=800,\n",
    "                  height=400,\n",
    "                  max_words=10,\n",
    "                  colormap='tab10',\n",
    "                  max_font_size=20,\n",
    "                  color_func=lambda *args, **kwargs: cols[i],\n",
    "                  prefer_horizontal=1.0)\n",
    "\n",
    "topics = lda_model.show_topics(formatted=False)\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(10,10), sharex=True, sharey=True)\n",
    "\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "    fig.add_subplot(ax)\n",
    "    topic_words = dict(topics[i][1])\n",
    "    cloud.generate_from_frequencies(topic_words, max_font_size=300)\n",
    "    plt.gca().imshow(cloud)\n",
    "    plt.gca().set_title('Topic ' + str(i), fontdict=dict(size=16))\n",
    "    plt.gca().axis('off')\n",
    "\n",
    "\n",
    "plt.subplots_adjust(wspace=0, hspace=0)\n",
    "plt.axis('off')\n",
    "plt.margins(x=0, y=0)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b1cb76e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_bag_of_words(topics, i):\n",
    "    topic_words = dict(topics[i][1])\n",
    "    wcloud = cloud.generate_from_frequencies(topic_words, max_font_size=300)\n",
    "    return hv.RGB(wcloud.to_array()).opts(xaxis=None, yaxis=None)\n",
    "\n",
    "plot_bag_of_words(topics, 0) + plot_bag_of_words(topics, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "800409cb",
   "metadata": {},
   "source": [
    "## 11. Sentence Chart Colored by Topic\n",
    "Each word in the document is representative of one of the 4 topics. Let’s color each word in the given documents by the topic id it is attributed to.\n",
    "The color of the enclosing rectangle is the topic assigned to the document."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c3723a0",
   "metadata": {},
   "source": [
    "## 14. Compute Model Perplexity and Coherence Score\n",
    "\n",
    "Model perplexity and topic coherence provide a convenient measure to judge how good a given topic model is. In my experience, topic coherence score, in particular, has been more helpful.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b5c184",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Perplexity\n",
    "print('\\nPerplexity: ', lda_model.log_perplexity(corpus))  # a measure of how good the model is. lower the better.\n",
    "\n",
    "# Compute Coherence Score\n",
    "coherence_model_lda = CoherenceModel(model=lda_model, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95867ba3",
   "metadata": {},
   "source": [
    "There you have a coherence score of 0.53."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a778508c",
   "metadata": {},
   "source": [
    "## 15. Visualize the topics-keywords\n",
    "\n",
    "Now that the LDA model is built, the next step is to examine the produced topics and the associated keywords. There is no better tool than pyLDAvis package’s interactive chart and is designed to work well with jupyter notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af762f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the topics\n",
    "import pyLDAvis.gensim_models as gensimvis\n",
    "pyLDAvis.enable_notebook()\n",
    "vis = gensimvis.prepare(lda_model, corpus, id2word)\n",
    "vis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de60a5e6",
   "metadata": {},
   "source": [
    "### So how to infer pyLDAvis’s output?\n",
    "\n",
    "Each bubble on the left-hand side plot represents a topic. The larger the bubble, the more prevalent is that topic.\n",
    "\n",
    "A good topic model will have fairly big, non-overlapping bubbles scattered throughout the chart instead of being clustered in one quadrant.\n",
    "\n",
    "A model with too many topics, will typically have many overlaps, small sized bubbles clustered in one region of the chart.\n",
    "\n",
    "Alright, if you move the cursor over one of the bubbles, the words and bars on the right-hand side will update. These words are the salient keywords that form the selected topic.\n",
    "\n",
    "We have successfully built a good looking topic model.\n",
    "\n",
    "\n",
    "Given our prior knowledge of the number of natural topics in the document, finding the best model was fairly straightforward.\n",
    "\n",
    "Upnext, we will improve upon this model by using Mallet’s version of LDA algorithm and then we will focus on how to arrive at the optimal number of topics given any large corpus of text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc138016",
   "metadata": {},
   "source": [
    "## 16. How to find the optimal number of topics for LDA?\n",
    "\n",
    "My approach to finding the optimal number of topics is to build many LDA models with different values of number of topics (k) and pick the one that gives the highest coherence value.\n",
    "\n",
    "Choosing a ‘k’ that marks the end of a rapid growth of topic coherence usually offers meaningful and interpretable topics. Picking an even higher value can sometimes provide more granular sub-topics.\n",
    "\n",
    "If you see the same keywords being repeated in multiple topics, it’s probably a sign that the ‘k’ is too large.\n",
    "\n",
    "The compute_coherence_values() (see below) trains multiple LDA models and provides the models and their corresponding coherence scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccffd942",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.models as gensim_models\n",
    "def compute_coherence_values(dictionary, corpus, texts, limit, start=2, step=3):\n",
    "    \"\"\"\n",
    "    Compute c_v coherence for various number of topics\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    dictionary : Gensim dictionary\n",
    "    corpus : Gensim corpus\n",
    "    texts : List of input texts\n",
    "    limit : Max num of topics\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    model_list : List of LDA topic models\n",
    "    coherence_values : Coherence values corresponding to the LDA model with respective number of topics\n",
    "    \"\"\"\n",
    "    coherence_values = []\n",
    "    model_list = []\n",
    "    for num_topics in range(start, limit, step):\n",
    "        model = gensim.models.ldamodel.LdaModel(corpus=corpus, num_topics=num_topics, id2word=id2word)\n",
    "        model_list.append(model)\n",
    "        coherencemodel = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "        coherence_values.append(coherencemodel.get_coherence())\n",
    "\n",
    "    return model_list, coherence_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84219a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can take a long time to run.\n",
    "model_list, coherence_values = compute_coherence_values(dictionary=id2word, corpus=corpus, texts=data_lemmatized, start=2, limit=40, step=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f91cfc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show graph\n",
    "limit=40; start=2; step=6;\n",
    "x = range(start, limit, step)\n",
    "plt.plot(x, coherence_values)\n",
    "plt.xlabel(\"Num Topics\")\n",
    "plt.ylabel(\"Coherence score\")\n",
    "plt.legend((\"coherence_values\"), loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f14e0ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "hv.Bars((x,coherence_values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ae3ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the coherence scores\n",
    "for m, cv in zip(x, coherence_values):\n",
    "    print(\"Num Topics =\", m, \" has Coherence Value of\", round(cv, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d9984c",
   "metadata": {},
   "source": [
    "If the coherence score seems to keep increasing, it may make better sense to pick the model that gave the highest CV before flattening out. This is exactly the case here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "765228a9",
   "metadata": {},
   "source": [
    "So for further steps I will choose the model with 20 topics itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c5fb179",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the model and print the topics\n",
    "optimal_model = model_list[3]\n",
    "model_topics = optimal_model.show_topics(formatted=False)\n",
    "pprint(optimal_model.print_topics(num_words=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f9026aa",
   "metadata": {},
   "source": [
    "Those were the topics for the chosen LDA model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0df1436a",
   "metadata": {},
   "source": [
    "## 17. Finding the dominant topic in each sentence\n",
    "\n",
    "One of the practical application of topic modeling is to determine what topic a given document is about.\n",
    "\n",
    "To find that, we find the topic number that has the highest percentage contribution in that document.\n",
    "\n",
    "The format_topics_sentences() function below nicely aggregates this information in a presentable table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2200259",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_topics_sentences(ldamodel=lda_model, corpus=corpus, texts=data):\n",
    "    # Init output\n",
    "    sent_topics_df = pd.DataFrame()\n",
    "\n",
    "    # Get main topic in each document\n",
    "    for i, row in enumerate(ldamodel[corpus]):\n",
    "        row = sorted(row, key=lambda x: (x[1]), reverse=True)\n",
    "        # Get the Dominant topic, Perc Contribution and Keywords for each document\n",
    "        for j, (topic_num, prop_topic) in enumerate(row):\n",
    "            if j == 0:  # => dominant topic\n",
    "                wp = ldamodel.show_topic(topic_num)\n",
    "                topic_keywords = \", \".join([word for word, prop in wp])\n",
    "                sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
    "            else:\n",
    "                break\n",
    "    sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']\n",
    "\n",
    "    # Add original text to the end of the output\n",
    "    contents = pd.Series(texts)\n",
    "    sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)\n",
    "    return(sent_topics_df)\n",
    "\n",
    "\n",
    "df_topic_sents_keywords = format_topics_sentences(ldamodel=optimal_model, corpus=corpus, texts=data)\n",
    "\n",
    "# Format\n",
    "df_dominant_topic = df_topic_sents_keywords.reset_index()\n",
    "df_dominant_topic.columns = ['Document_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Text']\n",
    "\n",
    "# Show\n",
    "df_dominant_topic.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c2a43ab",
   "metadata": {},
   "source": [
    "## 18. Find the most representative document for each topic\n",
    "\n",
    "Sometimes just the topic keywords may not be enough to make sense of what a topic is about. So, to help with understanding the topic, you can find the documents a given topic has contributed to the most and infer the topic by reading that document. Whew!!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f5a887",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group top 5 sentences under each topic\n",
    "sent_topics_sorteddf_mallet = pd.DataFrame()\n",
    "\n",
    "sent_topics_outdf_grpd = df_topic_sents_keywords.groupby('Dominant_Topic')\n",
    "\n",
    "for i, grp in sent_topics_outdf_grpd:\n",
    "    sent_topics_sorteddf_mallet = pd.concat([sent_topics_sorteddf_mallet, \n",
    "                                             grp.sort_values(['Perc_Contribution'], ascending=[0]).head(1)], \n",
    "                                            axis=0)\n",
    "\n",
    "# Reset Index    \n",
    "sent_topics_sorteddf_mallet.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Format\n",
    "sent_topics_sorteddf_mallet.columns = ['Topic_Num', \"Topic_Perc_Contrib\", \"Keywords\", \"Text\"]\n",
    "\n",
    "# Show\n",
    "sent_topics_sorteddf_mallet.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "658a512e",
   "metadata": {},
   "source": [
    "The tabular output above actually has 20 rows, one each for a topic. It has the topic number, the keywords, and the most representative document. The Perc_Contribution column is nothing but the percentage contribution of the topic in the given document.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "871c687e",
   "metadata": {},
   "source": [
    "## 19. Topic distribution across documents\n",
    "\n",
    "Finally, we want to understand the volume and distribution of topics in order to judge how widely it was discussed. The below table exposes that information.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8fc2857",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of Documents for Each Topic\n",
    "topic_counts = df_topic_sents_keywords['Dominant_Topic'].value_counts()\n",
    "\n",
    "# Percentage of Documents for Each Topic\n",
    "topic_contribution = round(topic_counts/topic_counts.sum(), 4)\n",
    "\n",
    "# Topic Number and Keywords\n",
    "topic_num_keywords = df_topic_sents_keywords[['Dominant_Topic', 'Topic_Keywords']]\n",
    "\n",
    "# Concatenate Column wise\n",
    "df_dominant_topics = pd.concat([topic_num_keywords, topic_counts, topic_contribution], axis=1)\n",
    "\n",
    "# Change Column names\n",
    "df_dominant_topics.columns = ['Dominant_Topic', 'Topic_Keywords', 'Num_Documents', 'Perc_Documents']\n",
    "\n",
    "# Show\n",
    "df_dominant_topics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e6c285",
   "metadata": {},
   "source": [
    "## 20. Conclusion\n",
    "\n",
    "We started with understanding what topic modeling can do. We built a basic topic model using Gensim’s LDA and visualize the topics using pyLDAvis.\n",
    "\n",
    "Then we built mallet’s LDA implementation.\n",
    "\n",
    "\n",
    "You saw how to find the optimal number of topics using coherence scores and how you can come to a logical understanding of how to choose the optimal model.\n",
    "\n",
    "Finally we saw how to aggregate and present the results to generate insights that may be in a more actionable.\n",
    "\n",
    "Hope you enjoyed reading this. I would appreciate if you leave your thoughts in the comments section below.\n",
    "\n",
    "Get the notebook and start using the codes right-away!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98fa04cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
